Since C is such a popular language and pointer based bugs are both widespread and highly exploitable, there have been many efforts to introduce pointer safety to C, and many different approaches.

LLVM's address sanitizer \cite{llvmAddrSan, llvmAddrSanAlgo} can be considered state of the art and  is capable of detecting out-of-bounds accesses on the heap, stack and for globals, use-after-free, and some use-after return bugs.

It does this by creating a copy of memory, called shadow memory, where 1 byte of shadow memory maps to 8 bytes of real memory.
This takes advantage of the fact that \verb!malloc! is guaranteed to return an 8-byte aligned segment of memory, therefore a value of 0 in shadow memory means the corresponding main memory is valid, a negative value means the corresponding main memory is invalid and a positive value of $n$ means the first $n$ bytes are valid and the rest are invalid.

The \verb!malloc! and \verb!free! functions are modified so as to mark the shadowed areas of memory as valid and poisoned respectively.
Additionally, \verb!malloc! is modified so that the memory surrounding that allocated to the program is poisoned to prevent overflows.

However, address sanitizer provides no mapping between the valid areas of memory and variables.
It would be possible for pointer arithmetic to be used to to still cause a buffer overflow into another variables' valid area of memory, though it must jump over the poisoned area.

Hardbound criticizes prior work, saying that it either introduces high overhead, isn't complete or introduces incompatibility \cite{devietti2008hardbound}.
In contrast it proposes to shift the bounds checking to hardware, and akin to SoftBound stores the bounds information separately from the pointer.

\section{The Jones and Kelly System}

An example of the table based approach is the Jones and Kelly approach \cite{jones1997backwards}.
At runtime an ordered list of objects in memory is maintained by tracking the uses of \verb!malloc! and \verb!free!.
It makes use of the fact that every valid pointer-valued expression in C derives its results from exactly one original storage object.
During pointer arithmetic, the referent object (the object pointed to) is identified in the object list using the operand pointer.
The bounds information is retrieved from the object list and used to check if the result is in bounds.

The Jones and Kelly bounds checker uses a strict interpretation of the C standard, where pointers cannot point to invalid memory areas.
Therefore out of bounds pointers are marked as such in a non-recoverable way (they are set equal to \verb!-2!).
To account for the standard allowed practice of generating a pointer pointing one past the end of an array, the bounds checker increases all arrays by 1 element.

In \textit{A Practical Dynamic Buffer Overflow Detector} \cite{ruwase2004practical} it was found that 60\% of programs tested did not adhere to the C-standard assumed in the Jones and Kelly approach and were therefore broken by the tactic of signifying illegal pointers by setting them to \verb!-2!.

To combat this, they created a new approach, where the creation of an out of bounds pointer would result in the creation of an Out Of Bounds object created on the heap which contains the address of the pointer and the referent object originally pointed to.
These Out Of Bounds objects are stored in a hash table.
Therefore on dereference, both the object list and the out of bounds hash table may be consulted to determine the validity of the pointer.
In order to reduce the overhead from these two lookups, only strings are bounds checked on the rationale that they are the tool used in buffer overflow attacks.

\textit{Baggy Bounds Checking} \cite{akritidis2009baggy} is an alternate optimization of the Jones and Kelly system, based on reducing the lookup time.
On a memory allocation, the size of the object is padded to the next power of two, enabling the size of the allocated memory to be stored more compactly as $lg_2(\mbox{size})$ taking the size of a single byte.
Due to the lower memory overhead of a entry, a constant sized array is used instead of an object list.
This allows a quick and constant time address calculation to be performed.
Alternate methods are used for dealing with pointers pointing past the end of arrays as adding one element to an array could double the size it could take up.

This approach does not prevent out of bounds accesses as the size associated with the pointer (the allocated bounds) is larger than the size of the object (the object bounds), so it is still possible to exceed the bounds of the object.
However it prevents dangerous overflows, since a pointer cannot access memory of an object that it was not created for.

\section{Other Systems}

\textit{Efficient Detection of all pointer and array access errors} \cite{austin1994efficient} uses a more complex fat pointer representation than just \{value, base, bound\} to extend their coverage beyond just spatial safety to include temporal safety.

The first additional field is the storage class enumeration, ranging over the values of Heap, Global and Local.
This allows detection of erroneous deallocations (such as attempting to free a local varaible).
The second field, a capability is more interesting.
On memory allocation, a unique capability is created and stored in a capability table.
The capability is deallocated once the memory is freed (either through free or returning from a function).
The presence of the capability referenced by a fat pointer is checked on pointer dereference.
This was found to produce an overhead of between 130-450\%.

\textit{Cyclone} \cite{jim2002cyclone} takes a different route from source code analysis, being a dialect of C that allows the programmer to program in a C-like language that prevents buffer overflows, memory managements and format string attacks.
It does this by imposing restrictions on C, such as limiting pointer arithmetic, disallowing unsafe casts and forbidding jumps into scopes.
Additionally it provides extensions such as a never-null pointer type and fat pointer for arithmetic.

While some of these extensions are automatic, the programmer must make use of most of them explicitly, so existing programs must be converted to Cyclone.
When tested, Cylcone produce overheads of between 0 and 250\% while the translation required between 0 and 46\% of the lines of code to be changed.

On the other end of the spectrum are tools that don't even need access to the source code of the program.

\textit{Body Armour for Binaries} \cite{slowinska2012body} targets a very specific type of attack - buffer overflows into non-control data, without requiring the source code or symbol table of the program.
It essentially does this by reverse engineering the binary to extract information about the data structures that need protecting and rewriting it to contain checks on pointer dereference.

\textit{Heapmon} \cite{shetty2005heapmon} deploys a helper thread that stores two bits for every word on the heap to keep track of whether or not the area is allocated and whether or not the area is initialized.
Memory leaks are therefore detected by looking for areas of allocated memory left over after the program exits.

To detect overflows, memory allocation is modified to leave unallocated areas between objects, so writing to that area will trigger an error.
Heapmon can only deal with memory on the heap (not the stack or globals) and works at a word granularity, so errors of less than 3 bytes may not be detected.

\section{SoftBound}

One of the two main systems implemented in this dissertation is SoftBound \cite{nagarakatte2009softbound}.
It is a compile-time transformation that stores information about the valid area of memory associated with a pointer separately from the pointer.

By storing information separately from the pointer, memory layout doesn't change, enabling binary compatibility and reducing implementation effort, however it does require a search for suitable bounds information on pointer dereference.
Additionally the paper contains a proof that spatial integrity is provided by checking the bounds of pointers on a store or load.

\section{CCured Analysis}

CCured designates pointer as one of three types: \textit{SAFE}, \textit{SEQ} and \textit{DYNAMIC}.
\textbf{Explain what each of these mean}
While the CCured language itself has pointers explicitly marked as one of these types, its main goal is to allow the use of CCured with existing unmodified C programs and it uses a type inference algorithm to do so.

Every pointer is annotated with a *qualifier variable*, which is one of the three above types.
We'll use $Q(a)$ to mean the qualifier variable of $a$ and $T(a)$ to mean the type pointed to by $a$.
These can be recursive, with the type pointed to by $a$ being a pointer itself, for example $a$ could be of type $int\; \mbox{ref}\; SAFE\; \mbox{ref}\; SAFE$, in which case $T(a) = int\;\mbox{ref}\;SAFE$.

\subsection{CCured Constraint Collection}

There are three operations that generate constraints in a C program, these are arithmetic, casting and assignment.

Arithmetic is the simplest one of these, and says that if a pointer has arithmetic performed on it, it cannot be \textit{SAFE}.

\begin{minted}[frame=single]{c}
int *a;
int *b;

a = b + 4;
\end{minted}

Two constraints are generated here, one from the arithmetic and one from the assignment.
The arithmetic constraint marks the pointer that has arithmetic performed on it, so in this case, $Q(b) != SAFE$.

Assignment is slightly more complicated, the assignment \verb|a = b| generates the following contraints if both a and b are pointers:

\begin{verbatim}
Q(a) = Q(b) = DYNQ 
\/ (
       (Q(a) = Q(b) = SAFE 
       \/ Q(a) = Q(b) = SEQ
       \/ Q(a) = SAFE /\ Q(b) = SEQ) 
    \/ T(a) = T(b)
)
\end{verbatim}

The first line is a provision that a \textit{DYNAMIC} pointer can be set to a \textit{DYNAMIC} pointer regardless of the types.

The remaining lines allow the matching up of qualifiers given if the types pointed to are equal.
A \textit{SAFE} pointer can be set to a \textit{SAFE} pointer, a \textit{SEQ} pointer can be set to a \textit{SEQ} pointer and a \textit{SAFE} pointer can be set to a \textit{SEQ} pointer.
In the last case, a bounds check is performed to ensure the safe pointer is set to a value within the bounds of the sequential pointer.
After the assignment, the safe pointer contains no further bounds information.

One final case exists for the assignment of \verb|a = b|, where $b$ is an integer and $b$ is a pointer.
In this case, the only constraint generated is $Q(e) != SAFE$.

Similar constraints are generated on a cast.

After the source code is iterated through and all of the constraints are generated, a constraint solver is run and the qualifiers for all variables.



\section{Relation to Work Done}

During the course of this project, two different LLVM transformation passes were developed.
The first uses a fat pointer representation to keep track of the base and bounds information for allocated memory areas, whereas the second keeps this data separate from the pointers themselves.

Therefore, both approaches described in this dissertation use the same method for ensuring temporal pointer safety - checking of base and bounds.
However the way they store the bounds information is different.

This should result in similar strengths and failings in terms of capability (what violations are caught), but different trade-offs in terms of performance.

