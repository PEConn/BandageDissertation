%% 
%% ACS project dissertation template. 
%% 
%% Currently designed for printing two-sided, but if you prefer to 
%% print single-sided just remove ",twoside,openright" from the 
%% \documentclass[] line below. 
%%
%%
%%   SMH, May 2010. 


\documentclass[a4paper,12pt,twoside,openright]{report}


%%
%% EDIT THE BELOW TO CUSTOMIZE
%%

\def\authorname{Peter E.\ Conn\xspace}
\def\authorcollege{Trinity Hall\xspace}
\def\authoremail{pc424@cl.cam.ac.uk}
\def\dissertationtitle{A Safety Enhancing Source Translation for C}
\def\wordcount{xx,xxx}

\usepackage{url}
\usepackage{epstopdf}
\usepackage{epsfig,graphicx,parskip,setspace,tabularx,xspace} 
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{minted}
\usetikzlibrary{shapes, arrows}

%% START OF DOCUMENT
\begin{document}


%% FRONTMATTER (TITLE PAGE, DECLARATION, ABSTRACT, ETC) 
\pagestyle{empty}
\singlespacing
\input{titlepage}
\onehalfspacing
\input{declaration}
\singlespacing
\newpage
{\Huge \bf Abstract}
\vspace{24pt} 

An LLVM implementation of fat pointers, pointers that store information additional to their value was created and evaluated as an optimisation pass for C.
Likewise, a meta-data approach, associating information separate from pointers with pointers was implemented with multiple back-ends for the storage of metadata.
Both methods were designed to provide spatial pointer safety.
Finally, a CCured-like analysis was implemented as an optimization in a further LLVM pass to identify and prevent unnessecary bounds checks.

The completeness of each approach was tested and the relative advantages and disadvantages were weighted.
The performance penalties caused by both approaches were evaluated and the speedup given by the CCured-analysis was investigated.
In addition, this is the first time CCured-like analysis has been used to augment a fat pointer approach to bounds checking.

\newpage
\vspace*{\fill}

\pagenumbering{roman}
\setcounter{page}{0}
\pagestyle{plain}
\tableofcontents
\listoffigures
\listoftables

\onehalfspacing

%% START OF MAIN TEXT 

\chapter{Introduction}
\pagenumbering{arabic} 
\setcounter{page}{1} 

Pointer errors account for many bugs in C, ranging from simple off-by-one errors where the programmer writes one past the end of the array to malicious buffer overflow attacks, where an attacker inputs data designed to manipulate the code into writing past the end of the array and over some other important data.

C allows many practices that make such errors easy to make, for example a pointer can legally point one past the end of an array (although dereferencing it is undefined).
Also, in practice, a pointer frequently starts pointing to its valid area of memory and then is modified (through pointer arithmetic) to point to an invalid area of memory.
This pointer is modified again, bringing it back to point to the valid area and used.

One method of dealing with such attacks is to keep track of data about the valid area of memory that a pointer has access to, and consult this data on pointer dereference.
In this dissertation, the data used to determine the validity of a pointer is the address of the start of the area of allocated memory (the base) and the address one past its end (the bound).
Furthermore, the base and bound data must be associated with the pointer in some way.

One aim of this dissertation is to investigate the runtime, completeness and implementation tradeoffs of two different methods of associating the data with the pointers.
With fat pointers, pointers themselves are replaced by structures that contain the pointer value and their bounds information.
With a metadata approach, the address of the pointer is used as the key to a lookup table containing the bounds information.

In addition, many pointers are not modified using pointer arithmetic during the life of the program and a blanket strategy of creating a bounds check on every pointer dereference can introduce a large, unnecessary overhead.
To this end, a method of identifying pointers that do not require bounds checking, CCured-like analysis was implemented and the savings this optimization cause are investigated.

\begin{figure}
\centering
\tikzstyle{block} = [rectangle, draw, 
text width=5em, text centered, rounded corners, minimum height=2em]
\tikzstyle{lib} = [rectangle, draw, fill=green!40, 
text width=5em, text centered, rounded corners, minimum height=2em]
\tikzstyle{comp} = [rectangle, draw, fill=blue!40, 
text width=5em, text centered, rounded corners, minimum height=2em]
\tikzstyle{line} = [draw, -latex']
\begin{tikzpicture}[node distance = 3cm, auto]
	\node [block] 						(code) 		{Source Code};
	\node [comp, right of=code] 		(ccured) 	{CCured-like Analysis};
	\node [comp, above of=ccured] 		(bandage) 	{Bandage};
	\node [block, right of=bandage] 	(exe1) 		{Bounds Checked Binary};
	\node [comp, below of=ccured] 		(softbound) {SoftBound};
	\node [block, right of=softbound] 	(exe2) 		{Bounds Checked Binary};
	\node [lib, below right of=exe2]	(hash)		{HashTable};
	\node [lib, above right of=exe2]	(mem)		{MemTable};
	\path[line] (code) -- (ccured);
	\path[line] (ccured) -- (bandage);
	\path[line] (ccured) -- (softbound);
	\path[line] (code) -- (bandage);
	\path[line] (code) -- (softbound);
	\path[line] (bandage) -- (exe1);
	\path[line] (softbound) -- (exe2);
	\path[line] (mem) -- (exe2);
	\path[line] (hash) -- (exe2);
\end{tikzpicture}
\caption{Overview of Implemented Components}
\label{fig:Components}
\end{figure}

Figure \ref{fig:Components} summarizes the components of the implementation.
Blue boxes represent LLVM passes and green boxes represent created object files that can be linked with the executable to provide different implementations of the same functionality.

\textbf{Pointers from library functions}

\textbf{Highlights}

This dissertation continues with brief coverage of the background information required: the types of pointer safety, examples of vulnerabilities caused by lack of it and an overview of how pointers are dealt with in LLVM IR.
A related work section follows, enumerating previous efforts to provide pointer safety, drawing together common themes between them and highlighting the position of this work.

The design and implementation section contains details of the two transformation passes (the fat pointer and the metadata) and the one analysis pass (the CCured-like analysis).
The evaluation section covers the runtime penalties, tested on handcrafted microbenchmarks and the olden benchmark suite, alongside performance security focussed correctness tests.
Everything is wrapped up in the conclusion and a few example source programs are annotated in the appendices to give a concrete view of the transformations performed.

\chapter{Background} 
\input{Background/Background}

\chapter{Related Work} 
\input{RelatedWork/RelatedWork}

\chapter{Design and Implementation} 
\input{Implementation/Implementation}

\chapter{Evaluation} 
\input{Evaluation/Evaluation}

\chapter{Summary and Conclusions} 

\section{Software Engineering Practices}

The use of a strong test suite was invaluable during the implementation, and the only thing I would change if I did this again would be making it even stronger.

In terms of implementation costs, implementing Bandage took considerably more effort and was far more bug prone than implementing SoftBound.
This is in large part due to Bandage changing the representation of pointers - making the change from raw pointers to fat pointers required a considerable amount of code to be written in one piece - if the entire change did not work, the source programs would break.
In the SoftBound implementation however, the implementation could be broken down into small pieces (eg providing bounds for local variables, then passing them through functions), and at each stage the source programs would compile, though the checking may not be complete.

Although this does provide a nice guarantee with Bandage that if the program compiles, bounds checking is more likely to be thorough, it creates an all-or-nothing situation in the presence of cases that haven't been implemented where Bandage either breaks the code or performs well, while Softbound can provide bounds checking for that that has been implemented.

Initially, only the Bandage system was to be implemented.
Therefore keeping the Bandage transformation pass separate from the CCured-like analysis pass proved beneficial as it could be easily reused to provide the same information to the SoftBound transformation.

Unexpected setbacks were discovered in trying to run the Softbound transformation on the Raspberry Pi, as both uthash and memtables required modification to run on 32-bit systems.

\appendix
\singlespacing

\bibliographystyle{unsrt} 
\bibliography{Bibliography} 

\end{document}
